{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic Regression, SVM and Deeplearning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this noteebook we apply the Logistic Regression, Support Vector Machines and Multilayer Perceptron algorithms to classify the credit risk of the credit database: https://www.kaggle.com/datasets/laotse/credit-risk-dataset \n"
      ],
      "metadata": {
        "id": "T_wT3-6lohbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Logistic Regression**"
      ],
      "metadata": {
        "id": "b7MxZFJs4afJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "2v9_p0RPtaHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UZwnQ-QiDHR"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression  #The logistic regression method"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle \n",
        "with open('credit.pkl', 'rb') as f:  \n",
        "  X_credit_treinamento, y_credit_treinamento, X_credit_teste, y_credit_teste = pickle.load(f)   #oppening the files"
      ],
      "metadata": {
        "id": "LHEyjahJup3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_credit_treinamento.shape, y_credit_treinamento.shape   #training dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RquO85uPu93J",
        "outputId": "92fef941-25f4-4da2-c17d-bf7764ae35af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1500, 3), (1500,))"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_credit_teste.shape, y_credit_teste.shape    #test dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HpN1W2Nu90c",
        "outputId": "b58faf44-1eb5-4259-bdb5-806375013f4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((500, 3), (500,))"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_credit = LogisticRegression(random_state=1)  #calling the logistic regression method\n",
        "logistic_credit.fit(X_credit_treinamento, y_credit_treinamento)  #applying first to our training dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUKa2IP9u9xv",
        "outputId": "f6c76ae4-3c19-40ca-edfa-59b02bbc829f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(random_state=1)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_credit.intercept_   #the value of the B0 parameter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xuiqckZu9sw",
        "outputId": "716a0835-5c55-41f6-8435-044999679adb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-6.02976095])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_credit.coef_     # the values of the parameter B1,B2,B3 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SIKKiWQu9qI",
        "outputId": "de1b2bf1-fe8c-4e73-ebfc-e83d660ac6be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2.54927091, -3.72279861,  3.93940349]])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previsoes = logistic_credit.predict(X_credit_teste)   #testing the trained algorithm\n",
        "previsoes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2B6B1NV7up1E",
        "outputId": "e948aaa2-4d93-4ede-a0bb-1ad1dee39c2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
              "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_credit_teste  # actual values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhNGK_-Qupys",
        "outputId": "a9cb5f48-c586-4585-9ab9-eb0f4a6a9b38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report  # measuring the accuracy of the Logistic Regression\n",
        "accuracy_score(y_credit_teste, previsoes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wpgog8zpupwR",
        "outputId": "6c77c85c-2d61-45f6-ad3e-a204c4a5a4e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.946"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "cm = ConfusionMatrix(logistic_credit)\n",
        "cm.fit(X_credit_treinamento, y_credit_treinamento)\n",
        "cm.score(X_credit_teste, y_credit_teste)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "dMT5V2Awupt1",
        "outputId": "e8f8f257-7ab8-4472-a8b4-ae7cc289403f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.946"
            ]
          },
          "metadata": {},
          "execution_count": 78
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_credit_teste, previsoes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTXGZ5t3uprZ",
        "outputId": "f8dc4dd3-33d9-47df-fdae-76a4be061c46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97       436\n",
            "           1       0.79      0.78      0.79        64\n",
            "\n",
            "    accuracy                           0.95       500\n",
            "   macro avg       0.88      0.88      0.88       500\n",
            "weighted avg       0.95      0.95      0.95       500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Support Vector Machines**"
      ],
      "metadata": {
        "id": "fB4o4Ss_4fI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC  # the SVM method"
      ],
      "metadata": {
        "id": "s1cLc1od4kMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle     #importing the data \n",
        "with open('credit.pkl', 'rb') as f:\n",
        "  X_credit_treinamento, y_credit_treinamento, X_credit_teste, y_credit_teste =pickle.load(f)"
      ],
      "metadata": {
        "id": "7PUYf77u5A3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " X_credit_treinamento.shape, y_credit_treinamento.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRc2Kjv85A02",
        "outputId": "f206f04c-925c-4293-b3b2-9140ea95b8f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1500, 3), (1500,))"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "svm_credit = SVC(kernel= 'rbf', random_state =1, C=2.0) #for better results, try different kernels, 'rbf', 'poly', 'sigmoid', 'linear', and different values for C\n",
        "svm_credit.fit(X_credit_treinamento, y_credit_treinamento)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ql8Em3X5Ays",
        "outputId": "807ae2f0-1cc3-4e31-8bce-2a489d8498ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=2.0, random_state=1)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previsoes = svm_credit.predict(X_credit_teste)\n",
        "previsoes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y24Lb0tQ5-xY",
        "outputId": "264a6ce6-92d7-4a35-cb20-02eb9e51f1e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_credit_teste"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9Jnhhtd5-u-",
        "outputId": "3f217e23-187b-4c0e-d4e9-8cb6c5cfc401"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "accuracy_score(previsoes, y_credit_teste)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqJpr__f5-si",
        "outputId": "703630ac-e1f7-4ca6-f824-75eb186fec93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.988"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "cm=ConfusionMatrix(svm_credit)\n",
        "cm.fit(X_credit_treinamento, y_credit_treinamento)\n",
        "cm.score(X_credit_teste,y_credit_teste)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "IAc9opmY5-qN",
        "outputId": "a7067efe-def2-461f-c90e-83afb360caec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.988"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_credit_teste, previsoes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiPHHn8b5-oJ",
        "outputId": "eba4418e-14d9-47d2-cbdf-996b54d09156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99       436\n",
            "           1       0.97      0.94      0.95        64\n",
            "\n",
            "    accuracy                           0.99       500\n",
            "   macro avg       0.98      0.97      0.97       500\n",
            "weighted avg       0.99      0.99      0.99       500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Deeplearning**"
      ],
      "metadata": {
        "id": "UWPJ0-P9WYs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier  #Multilayer perceptron"
      ],
      "metadata": {
        "id": "gCRtJLpjWc_p"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('credit.pkl','rb') as f:\n",
        "   X_credit_treinamento, y_credit_treinamento, X_credit_teste, y_credit_teste = pickle.load(f)"
      ],
      "metadata": {
        "id": "Uu_oxrU4Wc9Z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_credit_treinamento.shape, y_credit_treinamento.shape, X_credit_teste.shape, y_credit_teste.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ck-mJn8Wc7V",
        "outputId": "38d05f98-b14a-4cc5-bfb7-552f5eb4d15d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1500, 3), (1500,), (500, 3), (500,))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rede_neural_credit =MLPClassifier(max_iter=1500, verbose=True, tol =0.0000100, \n",
        "                                  solver='adam', \n",
        "                                  activation='relu', \n",
        "                                  hidden_layer_sizes=(2,2))  #two hidden layers with two neurons each\n",
        "rede_neural_credit.fit(X_credit_treinamento, y_credit_treinamento)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQ-I6ftbWc4n",
        "outputId": "570e23e5-9e48-40fc-c879-bcbbcc02a677"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.42018033\n",
            "Iteration 2, loss = 0.41896749\n",
            "Iteration 3, loss = 0.41780227\n",
            "Iteration 4, loss = 0.41664067\n",
            "Iteration 5, loss = 0.41557096\n",
            "Iteration 6, loss = 0.41452135\n",
            "Iteration 7, loss = 0.41354181\n",
            "Iteration 8, loss = 0.41246751\n",
            "Iteration 9, loss = 0.41151175\n",
            "Iteration 10, loss = 0.41056300\n",
            "Iteration 11, loss = 0.40955667\n",
            "Iteration 12, loss = 0.40861280\n",
            "Iteration 13, loss = 0.40768676\n",
            "Iteration 14, loss = 0.40676517\n",
            "Iteration 15, loss = 0.40580861\n",
            "Iteration 16, loss = 0.40486544\n",
            "Iteration 17, loss = 0.40391916\n",
            "Iteration 18, loss = 0.40296484\n",
            "Iteration 19, loss = 0.40199454\n",
            "Iteration 20, loss = 0.40094260\n",
            "Iteration 21, loss = 0.39985326\n",
            "Iteration 22, loss = 0.39881041\n",
            "Iteration 23, loss = 0.39769511\n",
            "Iteration 24, loss = 0.39649803\n",
            "Iteration 25, loss = 0.39535441\n",
            "Iteration 26, loss = 0.39428722\n",
            "Iteration 27, loss = 0.39307125\n",
            "Iteration 28, loss = 0.39194241\n",
            "Iteration 29, loss = 0.39076768\n",
            "Iteration 30, loss = 0.38957551\n",
            "Iteration 31, loss = 0.38836210\n",
            "Iteration 32, loss = 0.38712996\n",
            "Iteration 33, loss = 0.38583530\n",
            "Iteration 34, loss = 0.38456043\n",
            "Iteration 35, loss = 0.38318983\n",
            "Iteration 36, loss = 0.38183793\n",
            "Iteration 37, loss = 0.38053545\n",
            "Iteration 38, loss = 0.37900289\n",
            "Iteration 39, loss = 0.37754454\n",
            "Iteration 40, loss = 0.37613112\n",
            "Iteration 41, loss = 0.37457104\n",
            "Iteration 42, loss = 0.37300941\n",
            "Iteration 43, loss = 0.37143410\n",
            "Iteration 44, loss = 0.36983182\n",
            "Iteration 45, loss = 0.36824343\n",
            "Iteration 46, loss = 0.36666260\n",
            "Iteration 47, loss = 0.36504618\n",
            "Iteration 48, loss = 0.36337613\n",
            "Iteration 49, loss = 0.36169985\n",
            "Iteration 50, loss = 0.35999463\n",
            "Iteration 51, loss = 0.35818969\n",
            "Iteration 52, loss = 0.35642574\n",
            "Iteration 53, loss = 0.35463504\n",
            "Iteration 54, loss = 0.35271750\n",
            "Iteration 55, loss = 0.35089998\n",
            "Iteration 56, loss = 0.34906238\n",
            "Iteration 57, loss = 0.34710963\n",
            "Iteration 58, loss = 0.34517206\n",
            "Iteration 59, loss = 0.34321331\n",
            "Iteration 60, loss = 0.34127107\n",
            "Iteration 61, loss = 0.33933663\n",
            "Iteration 62, loss = 0.33738671\n",
            "Iteration 63, loss = 0.33546469\n",
            "Iteration 64, loss = 0.33349137\n",
            "Iteration 65, loss = 0.33152558\n",
            "Iteration 66, loss = 0.32952816\n",
            "Iteration 67, loss = 0.32756908\n",
            "Iteration 68, loss = 0.32560403\n",
            "Iteration 69, loss = 0.32357680\n",
            "Iteration 70, loss = 0.32160507\n",
            "Iteration 71, loss = 0.31961895\n",
            "Iteration 72, loss = 0.31762706\n",
            "Iteration 73, loss = 0.31551597\n",
            "Iteration 74, loss = 0.31346679\n",
            "Iteration 75, loss = 0.31133915\n",
            "Iteration 76, loss = 0.30928407\n",
            "Iteration 77, loss = 0.30709953\n",
            "Iteration 78, loss = 0.30491067\n",
            "Iteration 79, loss = 0.30277871\n",
            "Iteration 80, loss = 0.30048886\n",
            "Iteration 81, loss = 0.29825206\n",
            "Iteration 82, loss = 0.29602887\n",
            "Iteration 83, loss = 0.29369732\n",
            "Iteration 84, loss = 0.29144225\n",
            "Iteration 85, loss = 0.28905622\n",
            "Iteration 86, loss = 0.28678797\n",
            "Iteration 87, loss = 0.28439623\n",
            "Iteration 88, loss = 0.28212264\n",
            "Iteration 89, loss = 0.27975369\n",
            "Iteration 90, loss = 0.27743989\n",
            "Iteration 91, loss = 0.27506117\n",
            "Iteration 92, loss = 0.27272614\n",
            "Iteration 93, loss = 0.27040272\n",
            "Iteration 94, loss = 0.26795920\n",
            "Iteration 95, loss = 0.26559174\n",
            "Iteration 96, loss = 0.26325691\n",
            "Iteration 97, loss = 0.26089887\n",
            "Iteration 98, loss = 0.25853644\n",
            "Iteration 99, loss = 0.25620991\n",
            "Iteration 100, loss = 0.25391680\n",
            "Iteration 101, loss = 0.25159492\n",
            "Iteration 102, loss = 0.24941257\n",
            "Iteration 103, loss = 0.24708738\n",
            "Iteration 104, loss = 0.24492333\n",
            "Iteration 105, loss = 0.24282554\n",
            "Iteration 106, loss = 0.24063242\n",
            "Iteration 107, loss = 0.23847779\n",
            "Iteration 108, loss = 0.23640402\n",
            "Iteration 109, loss = 0.23450489\n",
            "Iteration 110, loss = 0.23250840\n",
            "Iteration 111, loss = 0.23082354\n",
            "Iteration 112, loss = 0.22898703\n",
            "Iteration 113, loss = 0.22729701\n",
            "Iteration 114, loss = 0.22565854\n",
            "Iteration 115, loss = 0.22406273\n",
            "Iteration 116, loss = 0.22260991\n",
            "Iteration 117, loss = 0.22107922\n",
            "Iteration 118, loss = 0.21963008\n",
            "Iteration 119, loss = 0.21823461\n",
            "Iteration 120, loss = 0.21683177\n",
            "Iteration 121, loss = 0.21553746\n",
            "Iteration 122, loss = 0.21421613\n",
            "Iteration 123, loss = 0.21296085\n",
            "Iteration 124, loss = 0.21172599\n",
            "Iteration 125, loss = 0.21054714\n",
            "Iteration 126, loss = 0.20938681\n",
            "Iteration 127, loss = 0.20820571\n",
            "Iteration 128, loss = 0.20715811\n",
            "Iteration 129, loss = 0.20600018\n",
            "Iteration 130, loss = 0.20487693\n",
            "Iteration 131, loss = 0.20378965\n",
            "Iteration 132, loss = 0.20275309\n",
            "Iteration 133, loss = 0.20174814\n",
            "Iteration 134, loss = 0.20073214\n",
            "Iteration 135, loss = 0.19970636\n",
            "Iteration 136, loss = 0.19871649\n",
            "Iteration 137, loss = 0.19778733\n",
            "Iteration 138, loss = 0.19683897\n",
            "Iteration 139, loss = 0.19591725\n",
            "Iteration 140, loss = 0.19506584\n",
            "Iteration 141, loss = 0.19412927\n",
            "Iteration 142, loss = 0.19324153\n",
            "Iteration 143, loss = 0.19241314\n",
            "Iteration 144, loss = 0.19158405\n",
            "Iteration 145, loss = 0.19071360\n",
            "Iteration 146, loss = 0.18989694\n",
            "Iteration 147, loss = 0.18910540\n",
            "Iteration 148, loss = 0.18834422\n",
            "Iteration 149, loss = 0.18753291\n",
            "Iteration 150, loss = 0.18681216\n",
            "Iteration 151, loss = 0.18603997\n",
            "Iteration 152, loss = 0.18529048\n",
            "Iteration 153, loss = 0.18450368\n",
            "Iteration 154, loss = 0.18377967\n",
            "Iteration 155, loss = 0.18309612\n",
            "Iteration 156, loss = 0.18238345\n",
            "Iteration 157, loss = 0.18170247\n",
            "Iteration 158, loss = 0.18099940\n",
            "Iteration 159, loss = 0.18031311\n",
            "Iteration 160, loss = 0.17968768\n",
            "Iteration 161, loss = 0.17904570\n",
            "Iteration 162, loss = 0.17837173\n",
            "Iteration 163, loss = 0.17771627\n",
            "Iteration 164, loss = 0.17710046\n",
            "Iteration 165, loss = 0.17645478\n",
            "Iteration 166, loss = 0.17577588\n",
            "Iteration 167, loss = 0.17515840\n",
            "Iteration 168, loss = 0.17447702\n",
            "Iteration 169, loss = 0.17377829\n",
            "Iteration 170, loss = 0.17309230\n",
            "Iteration 171, loss = 0.17238802\n",
            "Iteration 172, loss = 0.17147735\n",
            "Iteration 173, loss = 0.17063285\n",
            "Iteration 174, loss = 0.16950755\n",
            "Iteration 175, loss = 0.16843111\n",
            "Iteration 176, loss = 0.16750959\n",
            "Iteration 177, loss = 0.16661802\n",
            "Iteration 178, loss = 0.16570750\n",
            "Iteration 179, loss = 0.16501086\n",
            "Iteration 180, loss = 0.16420946\n",
            "Iteration 181, loss = 0.16341939\n",
            "Iteration 182, loss = 0.16267072\n",
            "Iteration 183, loss = 0.16197423\n",
            "Iteration 184, loss = 0.16128449\n",
            "Iteration 185, loss = 0.16055866\n",
            "Iteration 186, loss = 0.15990300\n",
            "Iteration 187, loss = 0.15925101\n",
            "Iteration 188, loss = 0.15855424\n",
            "Iteration 189, loss = 0.15791856\n",
            "Iteration 190, loss = 0.15726952\n",
            "Iteration 191, loss = 0.15666806\n",
            "Iteration 192, loss = 0.15602884\n",
            "Iteration 193, loss = 0.15538240\n",
            "Iteration 194, loss = 0.15480273\n",
            "Iteration 195, loss = 0.15418483\n",
            "Iteration 196, loss = 0.15357930\n",
            "Iteration 197, loss = 0.15303006\n",
            "Iteration 198, loss = 0.15244967\n",
            "Iteration 199, loss = 0.15188045\n",
            "Iteration 200, loss = 0.15138410\n",
            "Iteration 201, loss = 0.15079131\n",
            "Iteration 202, loss = 0.15025089\n",
            "Iteration 203, loss = 0.14971308\n",
            "Iteration 204, loss = 0.14919783\n",
            "Iteration 205, loss = 0.14866718\n",
            "Iteration 206, loss = 0.14823683\n",
            "Iteration 207, loss = 0.14773214\n",
            "Iteration 208, loss = 0.14718375\n",
            "Iteration 209, loss = 0.14674933\n",
            "Iteration 210, loss = 0.14621391\n",
            "Iteration 211, loss = 0.14576156\n",
            "Iteration 212, loss = 0.14528654\n",
            "Iteration 213, loss = 0.14480048\n",
            "Iteration 214, loss = 0.14434964\n",
            "Iteration 215, loss = 0.14387679\n",
            "Iteration 216, loss = 0.14343595\n",
            "Iteration 217, loss = 0.14295679\n",
            "Iteration 218, loss = 0.14252408\n",
            "Iteration 219, loss = 0.14208234\n",
            "Iteration 220, loss = 0.14162126\n",
            "Iteration 221, loss = 0.14122043\n",
            "Iteration 222, loss = 0.14078802\n",
            "Iteration 223, loss = 0.14037578\n",
            "Iteration 224, loss = 0.13992569\n",
            "Iteration 225, loss = 0.13951191\n",
            "Iteration 226, loss = 0.13913666\n",
            "Iteration 227, loss = 0.13869855\n",
            "Iteration 228, loss = 0.13828272\n",
            "Iteration 229, loss = 0.13790661\n",
            "Iteration 230, loss = 0.13744927\n",
            "Iteration 231, loss = 0.13709606\n",
            "Iteration 232, loss = 0.13666453\n",
            "Iteration 233, loss = 0.13629841\n",
            "Iteration 234, loss = 0.13587515\n",
            "Iteration 235, loss = 0.13544916\n",
            "Iteration 236, loss = 0.13507569\n",
            "Iteration 237, loss = 0.13467831\n",
            "Iteration 238, loss = 0.13427331\n",
            "Iteration 239, loss = 0.13388092\n",
            "Iteration 240, loss = 0.13349657\n",
            "Iteration 241, loss = 0.13325810\n",
            "Iteration 242, loss = 0.13275619\n",
            "Iteration 243, loss = 0.13239156\n",
            "Iteration 244, loss = 0.13202802\n",
            "Iteration 245, loss = 0.13167068\n",
            "Iteration 246, loss = 0.13130943\n",
            "Iteration 247, loss = 0.13095960\n",
            "Iteration 248, loss = 0.13064138\n",
            "Iteration 249, loss = 0.13021485\n",
            "Iteration 250, loss = 0.12989464\n",
            "Iteration 251, loss = 0.12946533\n",
            "Iteration 252, loss = 0.12916100\n",
            "Iteration 253, loss = 0.12878561\n",
            "Iteration 254, loss = 0.12839106\n",
            "Iteration 255, loss = 0.12803387\n",
            "Iteration 256, loss = 0.12765446\n",
            "Iteration 257, loss = 0.12728104\n",
            "Iteration 258, loss = 0.12690858\n",
            "Iteration 259, loss = 0.12664346\n",
            "Iteration 260, loss = 0.12611796\n",
            "Iteration 261, loss = 0.12568268\n",
            "Iteration 262, loss = 0.12524129\n",
            "Iteration 263, loss = 0.12470222\n",
            "Iteration 264, loss = 0.12416849\n",
            "Iteration 265, loss = 0.12370106\n",
            "Iteration 266, loss = 0.12310676\n",
            "Iteration 267, loss = 0.12262303\n",
            "Iteration 268, loss = 0.12198749\n",
            "Iteration 269, loss = 0.12132706\n",
            "Iteration 270, loss = 0.12067144\n",
            "Iteration 271, loss = 0.12005294\n",
            "Iteration 272, loss = 0.11943114\n",
            "Iteration 273, loss = 0.11871028\n",
            "Iteration 274, loss = 0.11793740\n",
            "Iteration 275, loss = 0.11712453\n",
            "Iteration 276, loss = 0.11631728\n",
            "Iteration 277, loss = 0.11548397\n",
            "Iteration 278, loss = 0.11450542\n",
            "Iteration 279, loss = 0.11367654\n",
            "Iteration 280, loss = 0.11244162\n",
            "Iteration 281, loss = 0.11134390\n",
            "Iteration 282, loss = 0.11035117\n",
            "Iteration 283, loss = 0.10921549\n",
            "Iteration 284, loss = 0.10829443\n",
            "Iteration 285, loss = 0.10744087\n",
            "Iteration 286, loss = 0.10670433\n",
            "Iteration 287, loss = 0.10599996\n",
            "Iteration 288, loss = 0.10533264\n",
            "Iteration 289, loss = 0.10472330\n",
            "Iteration 290, loss = 0.10423236\n",
            "Iteration 291, loss = 0.10356071\n",
            "Iteration 292, loss = 0.10307248\n",
            "Iteration 293, loss = 0.10250137\n",
            "Iteration 294, loss = 0.10194762\n",
            "Iteration 295, loss = 0.10145223\n",
            "Iteration 296, loss = 0.10090240\n",
            "Iteration 297, loss = 0.10040385\n",
            "Iteration 298, loss = 0.09990502\n",
            "Iteration 299, loss = 0.09941309\n",
            "Iteration 300, loss = 0.09894147\n",
            "Iteration 301, loss = 0.09841168\n",
            "Iteration 302, loss = 0.09795762\n",
            "Iteration 303, loss = 0.09748343\n",
            "Iteration 304, loss = 0.09699847\n",
            "Iteration 305, loss = 0.09654875\n",
            "Iteration 306, loss = 0.09606597\n",
            "Iteration 307, loss = 0.09556417\n",
            "Iteration 308, loss = 0.09510492\n",
            "Iteration 309, loss = 0.09461122\n",
            "Iteration 310, loss = 0.09407489\n",
            "Iteration 311, loss = 0.09357671\n",
            "Iteration 312, loss = 0.09303687\n",
            "Iteration 313, loss = 0.09247620\n",
            "Iteration 314, loss = 0.09194441\n",
            "Iteration 315, loss = 0.09142040\n",
            "Iteration 316, loss = 0.09088994\n",
            "Iteration 317, loss = 0.09037672\n",
            "Iteration 318, loss = 0.08988333\n",
            "Iteration 319, loss = 0.08936558\n",
            "Iteration 320, loss = 0.08893913\n",
            "Iteration 321, loss = 0.08840195\n",
            "Iteration 322, loss = 0.08792083\n",
            "Iteration 323, loss = 0.08744804\n",
            "Iteration 324, loss = 0.08699438\n",
            "Iteration 325, loss = 0.08649378\n",
            "Iteration 326, loss = 0.08605754\n",
            "Iteration 327, loss = 0.08562200\n",
            "Iteration 328, loss = 0.08517091\n",
            "Iteration 329, loss = 0.08476310\n",
            "Iteration 330, loss = 0.08436054\n",
            "Iteration 331, loss = 0.08387882\n",
            "Iteration 332, loss = 0.08346698\n",
            "Iteration 333, loss = 0.08302900\n",
            "Iteration 334, loss = 0.08265831\n",
            "Iteration 335, loss = 0.08226772\n",
            "Iteration 336, loss = 0.08183519\n",
            "Iteration 337, loss = 0.08142795\n",
            "Iteration 338, loss = 0.08104650\n",
            "Iteration 339, loss = 0.08059837\n",
            "Iteration 340, loss = 0.08025058\n",
            "Iteration 341, loss = 0.07986972\n",
            "Iteration 342, loss = 0.07949935\n",
            "Iteration 343, loss = 0.07908292\n",
            "Iteration 344, loss = 0.07871541\n",
            "Iteration 345, loss = 0.07833363\n",
            "Iteration 346, loss = 0.07797772\n",
            "Iteration 347, loss = 0.07762119\n",
            "Iteration 348, loss = 0.07726127\n",
            "Iteration 349, loss = 0.07688102\n",
            "Iteration 350, loss = 0.07654793\n",
            "Iteration 351, loss = 0.07621256\n",
            "Iteration 352, loss = 0.07586615\n",
            "Iteration 353, loss = 0.07551912\n",
            "Iteration 354, loss = 0.07514714\n",
            "Iteration 355, loss = 0.07483836\n",
            "Iteration 356, loss = 0.07448042\n",
            "Iteration 357, loss = 0.07417261\n",
            "Iteration 358, loss = 0.07384400\n",
            "Iteration 359, loss = 0.07348825\n",
            "Iteration 360, loss = 0.07319837\n",
            "Iteration 361, loss = 0.07298222\n",
            "Iteration 362, loss = 0.07260132\n",
            "Iteration 363, loss = 0.07223761\n",
            "Iteration 364, loss = 0.07191128\n",
            "Iteration 365, loss = 0.07160682\n",
            "Iteration 366, loss = 0.07130889\n",
            "Iteration 367, loss = 0.07100708\n",
            "Iteration 368, loss = 0.07072941\n",
            "Iteration 369, loss = 0.07041206\n",
            "Iteration 370, loss = 0.07010493\n",
            "Iteration 371, loss = 0.07001381\n",
            "Iteration 372, loss = 0.06954918\n",
            "Iteration 373, loss = 0.06936457\n",
            "Iteration 374, loss = 0.06901612\n",
            "Iteration 375, loss = 0.06868196\n",
            "Iteration 376, loss = 0.06839676\n",
            "Iteration 377, loss = 0.06812251\n",
            "Iteration 378, loss = 0.06785375\n",
            "Iteration 379, loss = 0.06753105\n",
            "Iteration 380, loss = 0.06726689\n",
            "Iteration 381, loss = 0.06700386\n",
            "Iteration 382, loss = 0.06673247\n",
            "Iteration 383, loss = 0.06644471\n",
            "Iteration 384, loss = 0.06616907\n",
            "Iteration 385, loss = 0.06590517\n",
            "Iteration 386, loss = 0.06565317\n",
            "Iteration 387, loss = 0.06533238\n",
            "Iteration 388, loss = 0.06506720\n",
            "Iteration 389, loss = 0.06480439\n",
            "Iteration 390, loss = 0.06450874\n",
            "Iteration 391, loss = 0.06424028\n",
            "Iteration 392, loss = 0.06396183\n",
            "Iteration 393, loss = 0.06367400\n",
            "Iteration 394, loss = 0.06348934\n",
            "Iteration 395, loss = 0.06323686\n",
            "Iteration 396, loss = 0.06292183\n",
            "Iteration 397, loss = 0.06274207\n",
            "Iteration 398, loss = 0.06239299\n",
            "Iteration 399, loss = 0.06214537\n",
            "Iteration 400, loss = 0.06190644\n",
            "Iteration 401, loss = 0.06169762\n",
            "Iteration 402, loss = 0.06142437\n",
            "Iteration 403, loss = 0.06114542\n",
            "Iteration 404, loss = 0.06088149\n",
            "Iteration 405, loss = 0.06064718\n",
            "Iteration 406, loss = 0.06040273\n",
            "Iteration 407, loss = 0.06015055\n",
            "Iteration 408, loss = 0.05989767\n",
            "Iteration 409, loss = 0.05966045\n",
            "Iteration 410, loss = 0.05939330\n",
            "Iteration 411, loss = 0.05920960\n",
            "Iteration 412, loss = 0.05890129\n",
            "Iteration 413, loss = 0.05868208\n",
            "Iteration 414, loss = 0.05845959\n",
            "Iteration 415, loss = 0.05821853\n",
            "Iteration 416, loss = 0.05797286\n",
            "Iteration 417, loss = 0.05778432\n",
            "Iteration 418, loss = 0.05750567\n",
            "Iteration 419, loss = 0.05732184\n",
            "Iteration 420, loss = 0.05704736\n",
            "Iteration 421, loss = 0.05683477\n",
            "Iteration 422, loss = 0.05662236\n",
            "Iteration 423, loss = 0.05636216\n",
            "Iteration 424, loss = 0.05616062\n",
            "Iteration 425, loss = 0.05592197\n",
            "Iteration 426, loss = 0.05569698\n",
            "Iteration 427, loss = 0.05546732\n",
            "Iteration 428, loss = 0.05531381\n",
            "Iteration 429, loss = 0.05507257\n",
            "Iteration 430, loss = 0.05479548\n",
            "Iteration 431, loss = 0.05461670\n",
            "Iteration 432, loss = 0.05438441\n",
            "Iteration 433, loss = 0.05423299\n",
            "Iteration 434, loss = 0.05392214\n",
            "Iteration 435, loss = 0.05372104\n",
            "Iteration 436, loss = 0.05354938\n",
            "Iteration 437, loss = 0.05333314\n",
            "Iteration 438, loss = 0.05314334\n",
            "Iteration 439, loss = 0.05287876\n",
            "Iteration 440, loss = 0.05267366\n",
            "Iteration 441, loss = 0.05244584\n",
            "Iteration 442, loss = 0.05228040\n",
            "Iteration 443, loss = 0.05203733\n",
            "Iteration 444, loss = 0.05187190\n",
            "Iteration 445, loss = 0.05164815\n",
            "Iteration 446, loss = 0.05143199\n",
            "Iteration 447, loss = 0.05125804\n",
            "Iteration 448, loss = 0.05105275\n",
            "Iteration 449, loss = 0.05084377\n",
            "Iteration 450, loss = 0.05063052\n",
            "Iteration 451, loss = 0.05058505\n",
            "Iteration 452, loss = 0.05025182\n",
            "Iteration 453, loss = 0.05004493\n",
            "Iteration 454, loss = 0.04987575\n",
            "Iteration 455, loss = 0.04967585\n",
            "Iteration 456, loss = 0.04949276\n",
            "Iteration 457, loss = 0.04929925\n",
            "Iteration 458, loss = 0.04910591\n",
            "Iteration 459, loss = 0.04899934\n",
            "Iteration 460, loss = 0.04876129\n",
            "Iteration 461, loss = 0.04859784\n",
            "Iteration 462, loss = 0.04841979\n",
            "Iteration 463, loss = 0.04829151\n",
            "Iteration 464, loss = 0.04799706\n",
            "Iteration 465, loss = 0.04775940\n",
            "Iteration 466, loss = 0.04760542\n",
            "Iteration 467, loss = 0.04746458\n",
            "Iteration 468, loss = 0.04723900\n",
            "Iteration 469, loss = 0.04710281\n",
            "Iteration 470, loss = 0.04688131\n",
            "Iteration 471, loss = 0.04672490\n",
            "Iteration 472, loss = 0.04653482\n",
            "Iteration 473, loss = 0.04634772\n",
            "Iteration 474, loss = 0.04618165\n",
            "Iteration 475, loss = 0.04603264\n",
            "Iteration 476, loss = 0.04584687\n",
            "Iteration 477, loss = 0.04563124\n",
            "Iteration 478, loss = 0.04567937\n",
            "Iteration 479, loss = 0.04532254\n",
            "Iteration 480, loss = 0.04518835\n",
            "Iteration 481, loss = 0.04508268\n",
            "Iteration 482, loss = 0.04486054\n",
            "Iteration 483, loss = 0.04469199\n",
            "Iteration 484, loss = 0.04454587\n",
            "Iteration 485, loss = 0.04435161\n",
            "Iteration 486, loss = 0.04418019\n",
            "Iteration 487, loss = 0.04401121\n",
            "Iteration 488, loss = 0.04383007\n",
            "Iteration 489, loss = 0.04367205\n",
            "Iteration 490, loss = 0.04351558\n",
            "Iteration 491, loss = 0.04335509\n",
            "Iteration 492, loss = 0.04323378\n",
            "Iteration 493, loss = 0.04310960\n",
            "Iteration 494, loss = 0.04285869\n",
            "Iteration 495, loss = 0.04273618\n",
            "Iteration 496, loss = 0.04254611\n",
            "Iteration 497, loss = 0.04245606\n",
            "Iteration 498, loss = 0.04243819\n",
            "Iteration 499, loss = 0.04213549\n",
            "Iteration 500, loss = 0.04191954\n",
            "Iteration 501, loss = 0.04180739\n",
            "Iteration 502, loss = 0.04166286\n",
            "Iteration 503, loss = 0.04146609\n",
            "Iteration 504, loss = 0.04133352\n",
            "Iteration 505, loss = 0.04120326\n",
            "Iteration 506, loss = 0.04107588\n",
            "Iteration 507, loss = 0.04090338\n",
            "Iteration 508, loss = 0.04071033\n",
            "Iteration 509, loss = 0.04065521\n",
            "Iteration 510, loss = 0.04057679\n",
            "Iteration 511, loss = 0.04034044\n",
            "Iteration 512, loss = 0.04015608\n",
            "Iteration 513, loss = 0.04004817\n",
            "Iteration 514, loss = 0.03991525\n",
            "Iteration 515, loss = 0.03976397\n",
            "Iteration 516, loss = 0.03957071\n",
            "Iteration 517, loss = 0.03944425\n",
            "Iteration 518, loss = 0.03937063\n",
            "Iteration 519, loss = 0.03921727\n",
            "Iteration 520, loss = 0.03904047\n",
            "Iteration 521, loss = 0.03892237\n",
            "Iteration 522, loss = 0.03877266\n",
            "Iteration 523, loss = 0.03865917\n",
            "Iteration 524, loss = 0.03853367\n",
            "Iteration 525, loss = 0.03837853\n",
            "Iteration 526, loss = 0.03825557\n",
            "Iteration 527, loss = 0.03818674\n",
            "Iteration 528, loss = 0.03795972\n",
            "Iteration 529, loss = 0.03787519\n",
            "Iteration 530, loss = 0.03771892\n",
            "Iteration 531, loss = 0.03759863\n",
            "Iteration 532, loss = 0.03748782\n",
            "Iteration 533, loss = 0.03738642\n",
            "Iteration 534, loss = 0.03720933\n",
            "Iteration 535, loss = 0.03707394\n",
            "Iteration 536, loss = 0.03701250\n",
            "Iteration 537, loss = 0.03683985\n",
            "Iteration 538, loss = 0.03670992\n",
            "Iteration 539, loss = 0.03659443\n",
            "Iteration 540, loss = 0.03649462\n",
            "Iteration 541, loss = 0.03637152\n",
            "Iteration 542, loss = 0.03618755\n",
            "Iteration 543, loss = 0.03614681\n",
            "Iteration 544, loss = 0.03601792\n",
            "Iteration 545, loss = 0.03584104\n",
            "Iteration 546, loss = 0.03568437\n",
            "Iteration 547, loss = 0.03560030\n",
            "Iteration 548, loss = 0.03546889\n",
            "Iteration 549, loss = 0.03533647\n",
            "Iteration 550, loss = 0.03524944\n",
            "Iteration 551, loss = 0.03510631\n",
            "Iteration 552, loss = 0.03501757\n",
            "Iteration 553, loss = 0.03489894\n",
            "Iteration 554, loss = 0.03478717\n",
            "Iteration 555, loss = 0.03465088\n",
            "Iteration 556, loss = 0.03456373\n",
            "Iteration 557, loss = 0.03443001\n",
            "Iteration 558, loss = 0.03434925\n",
            "Iteration 559, loss = 0.03420364\n",
            "Iteration 560, loss = 0.03407172\n",
            "Iteration 561, loss = 0.03395944\n",
            "Iteration 562, loss = 0.03387224\n",
            "Iteration 563, loss = 0.03377566\n",
            "Iteration 564, loss = 0.03364889\n",
            "Iteration 565, loss = 0.03355340\n",
            "Iteration 566, loss = 0.03344275\n",
            "Iteration 567, loss = 0.03335415\n",
            "Iteration 568, loss = 0.03324040\n",
            "Iteration 569, loss = 0.03315375\n",
            "Iteration 570, loss = 0.03307176\n",
            "Iteration 571, loss = 0.03293088\n",
            "Iteration 572, loss = 0.03278057\n",
            "Iteration 573, loss = 0.03269597\n",
            "Iteration 574, loss = 0.03258608\n",
            "Iteration 575, loss = 0.03249765\n",
            "Iteration 576, loss = 0.03242944\n",
            "Iteration 577, loss = 0.03229278\n",
            "Iteration 578, loss = 0.03240508\n",
            "Iteration 579, loss = 0.03206346\n",
            "Iteration 580, loss = 0.03200087\n",
            "Iteration 581, loss = 0.03187366\n",
            "Iteration 582, loss = 0.03182151\n",
            "Iteration 583, loss = 0.03167293\n",
            "Iteration 584, loss = 0.03164712\n",
            "Iteration 585, loss = 0.03149700\n",
            "Iteration 586, loss = 0.03137820\n",
            "Iteration 587, loss = 0.03130844\n",
            "Iteration 588, loss = 0.03119839\n",
            "Iteration 589, loss = 0.03108372\n",
            "Iteration 590, loss = 0.03099075\n",
            "Iteration 591, loss = 0.03090026\n",
            "Iteration 592, loss = 0.03080756\n",
            "Iteration 593, loss = 0.03074396\n",
            "Iteration 594, loss = 0.03064317\n",
            "Iteration 595, loss = 0.03051569\n",
            "Iteration 596, loss = 0.03045334\n",
            "Iteration 597, loss = 0.03033912\n",
            "Iteration 598, loss = 0.03034032\n",
            "Iteration 599, loss = 0.03019239\n",
            "Iteration 600, loss = 0.03012168\n",
            "Iteration 601, loss = 0.02997902\n",
            "Iteration 602, loss = 0.02988485\n",
            "Iteration 603, loss = 0.02978791\n",
            "Iteration 604, loss = 0.02969729\n",
            "Iteration 605, loss = 0.02966449\n",
            "Iteration 606, loss = 0.02958976\n",
            "Iteration 607, loss = 0.02942637\n",
            "Iteration 608, loss = 0.02934514\n",
            "Iteration 609, loss = 0.02925130\n",
            "Iteration 610, loss = 0.02917147\n",
            "Iteration 611, loss = 0.02911536\n",
            "Iteration 612, loss = 0.02900813\n",
            "Iteration 613, loss = 0.02887978\n",
            "Iteration 614, loss = 0.02879450\n",
            "Iteration 615, loss = 0.02883794\n",
            "Iteration 616, loss = 0.02867983\n",
            "Iteration 617, loss = 0.02856233\n",
            "Iteration 618, loss = 0.02845801\n",
            "Iteration 619, loss = 0.02837227\n",
            "Iteration 620, loss = 0.02831815\n",
            "Iteration 621, loss = 0.02821612\n",
            "Iteration 622, loss = 0.02815534\n",
            "Iteration 623, loss = 0.02805634\n",
            "Iteration 624, loss = 0.02799598\n",
            "Iteration 625, loss = 0.02783628\n",
            "Iteration 626, loss = 0.02781475\n",
            "Iteration 627, loss = 0.02770783\n",
            "Iteration 628, loss = 0.02761051\n",
            "Iteration 629, loss = 0.02753417\n",
            "Iteration 630, loss = 0.02746985\n",
            "Iteration 631, loss = 0.02738106\n",
            "Iteration 632, loss = 0.02735605\n",
            "Iteration 633, loss = 0.02723966\n",
            "Iteration 634, loss = 0.02729685\n",
            "Iteration 635, loss = 0.02714763\n",
            "Iteration 636, loss = 0.02697149\n",
            "Iteration 637, loss = 0.02691431\n",
            "Iteration 638, loss = 0.02680750\n",
            "Iteration 639, loss = 0.02672417\n",
            "Iteration 640, loss = 0.02668436\n",
            "Iteration 641, loss = 0.02656738\n",
            "Iteration 642, loss = 0.02651139\n",
            "Iteration 643, loss = 0.02646633\n",
            "Iteration 644, loss = 0.02632327\n",
            "Iteration 645, loss = 0.02628951\n",
            "Iteration 646, loss = 0.02622135\n",
            "Iteration 647, loss = 0.02619999\n",
            "Iteration 648, loss = 0.02606337\n",
            "Iteration 649, loss = 0.02598263\n",
            "Iteration 650, loss = 0.02589154\n",
            "Iteration 651, loss = 0.02585050\n",
            "Iteration 652, loss = 0.02575202\n",
            "Iteration 653, loss = 0.02565421\n",
            "Iteration 654, loss = 0.02559150\n",
            "Iteration 655, loss = 0.02552713\n",
            "Iteration 656, loss = 0.02546735\n",
            "Iteration 657, loss = 0.02535693\n",
            "Iteration 658, loss = 0.02537930\n",
            "Iteration 659, loss = 0.02535748\n",
            "Iteration 660, loss = 0.02512265\n",
            "Iteration 661, loss = 0.02510090\n",
            "Iteration 662, loss = 0.02501160\n",
            "Iteration 663, loss = 0.02498809\n",
            "Iteration 664, loss = 0.02486368\n",
            "Iteration 665, loss = 0.02482033\n",
            "Iteration 666, loss = 0.02474679\n",
            "Iteration 667, loss = 0.02468952\n",
            "Iteration 668, loss = 0.02457740\n",
            "Iteration 669, loss = 0.02449703\n",
            "Iteration 670, loss = 0.02443373\n",
            "Iteration 671, loss = 0.02443653\n",
            "Iteration 672, loss = 0.02432670\n",
            "Iteration 673, loss = 0.02427915\n",
            "Iteration 674, loss = 0.02414008\n",
            "Iteration 675, loss = 0.02408161\n",
            "Iteration 676, loss = 0.02406461\n",
            "Iteration 677, loss = 0.02398678\n",
            "Iteration 678, loss = 0.02390776\n",
            "Iteration 679, loss = 0.02382410\n",
            "Iteration 680, loss = 0.02377000\n",
            "Iteration 681, loss = 0.02372335\n",
            "Iteration 682, loss = 0.02366990\n",
            "Iteration 683, loss = 0.02358307\n",
            "Iteration 684, loss = 0.02350265\n",
            "Iteration 685, loss = 0.02343420\n",
            "Iteration 686, loss = 0.02334771\n",
            "Iteration 687, loss = 0.02328035\n",
            "Iteration 688, loss = 0.02330947\n",
            "Iteration 689, loss = 0.02320737\n",
            "Iteration 690, loss = 0.02311854\n",
            "Iteration 691, loss = 0.02302875\n",
            "Iteration 692, loss = 0.02304927\n",
            "Iteration 693, loss = 0.02289291\n",
            "Iteration 694, loss = 0.02283307\n",
            "Iteration 695, loss = 0.02280661\n",
            "Iteration 696, loss = 0.02280376\n",
            "Iteration 697, loss = 0.02269674\n",
            "Iteration 698, loss = 0.02260623\n",
            "Iteration 699, loss = 0.02257998\n",
            "Iteration 700, loss = 0.02248252\n",
            "Iteration 701, loss = 0.02239593\n",
            "Iteration 702, loss = 0.02234249\n",
            "Iteration 703, loss = 0.02227921\n",
            "Iteration 704, loss = 0.02227107\n",
            "Iteration 705, loss = 0.02222160\n",
            "Iteration 706, loss = 0.02208247\n",
            "Iteration 707, loss = 0.02205734\n",
            "Iteration 708, loss = 0.02201147\n",
            "Iteration 709, loss = 0.02201570\n",
            "Iteration 710, loss = 0.02190101\n",
            "Iteration 711, loss = 0.02177740\n",
            "Iteration 712, loss = 0.02172494\n",
            "Iteration 713, loss = 0.02171899\n",
            "Iteration 714, loss = 0.02168388\n",
            "Iteration 715, loss = 0.02156817\n",
            "Iteration 716, loss = 0.02160137\n",
            "Iteration 717, loss = 0.02155666\n",
            "Iteration 718, loss = 0.02154562\n",
            "Iteration 719, loss = 0.02135384\n",
            "Iteration 720, loss = 0.02127914\n",
            "Iteration 721, loss = 0.02124240\n",
            "Iteration 722, loss = 0.02119167\n",
            "Iteration 723, loss = 0.02115243\n",
            "Iteration 724, loss = 0.02108687\n",
            "Iteration 725, loss = 0.02101361\n",
            "Iteration 726, loss = 0.02099921\n",
            "Iteration 727, loss = 0.02088184\n",
            "Iteration 728, loss = 0.02082237\n",
            "Iteration 729, loss = 0.02081332\n",
            "Iteration 730, loss = 0.02073244\n",
            "Iteration 731, loss = 0.02069341\n",
            "Iteration 732, loss = 0.02060330\n",
            "Iteration 733, loss = 0.02054987\n",
            "Iteration 734, loss = 0.02053022\n",
            "Iteration 735, loss = 0.02046742\n",
            "Iteration 736, loss = 0.02042738\n",
            "Iteration 737, loss = 0.02034426\n",
            "Iteration 738, loss = 0.02028583\n",
            "Iteration 739, loss = 0.02024444\n",
            "Iteration 740, loss = 0.02027144\n",
            "Iteration 741, loss = 0.02015323\n",
            "Iteration 742, loss = 0.02010967\n",
            "Iteration 743, loss = 0.02008442\n",
            "Iteration 744, loss = 0.02001290\n",
            "Iteration 745, loss = 0.01992095\n",
            "Iteration 746, loss = 0.01984641\n",
            "Iteration 747, loss = 0.01984794\n",
            "Iteration 748, loss = 0.01977589\n",
            "Iteration 749, loss = 0.01970538\n",
            "Iteration 750, loss = 0.01968646\n",
            "Iteration 751, loss = 0.01961573\n",
            "Iteration 752, loss = 0.01955915\n",
            "Iteration 753, loss = 0.01956186\n",
            "Iteration 754, loss = 0.01951246\n",
            "Iteration 755, loss = 0.01943669\n",
            "Iteration 756, loss = 0.01936821\n",
            "Iteration 757, loss = 0.01934396\n",
            "Iteration 758, loss = 0.01928648\n",
            "Iteration 759, loss = 0.01927862\n",
            "Iteration 760, loss = 0.01915233\n",
            "Iteration 761, loss = 0.01917560\n",
            "Iteration 762, loss = 0.01907356\n",
            "Iteration 763, loss = 0.01909629\n",
            "Iteration 764, loss = 0.01896001\n",
            "Iteration 765, loss = 0.01891227\n",
            "Iteration 766, loss = 0.01886331\n",
            "Iteration 767, loss = 0.01881725\n",
            "Iteration 768, loss = 0.01876501\n",
            "Iteration 769, loss = 0.01874180\n",
            "Iteration 770, loss = 0.01866737\n",
            "Iteration 771, loss = 0.01863778\n",
            "Iteration 772, loss = 0.01857446\n",
            "Iteration 773, loss = 0.01852461\n",
            "Iteration 774, loss = 0.01846202\n",
            "Iteration 775, loss = 0.01843607\n",
            "Iteration 776, loss = 0.01839509\n",
            "Iteration 777, loss = 0.01835283\n",
            "Iteration 778, loss = 0.01829005\n",
            "Iteration 779, loss = 0.01824700\n",
            "Iteration 780, loss = 0.01827091\n",
            "Iteration 781, loss = 0.01818723\n",
            "Iteration 782, loss = 0.01815345\n",
            "Iteration 783, loss = 0.01805654\n",
            "Iteration 784, loss = 0.01806235\n",
            "Iteration 785, loss = 0.01809460\n",
            "Iteration 786, loss = 0.01789360\n",
            "Iteration 787, loss = 0.01788582\n",
            "Iteration 788, loss = 0.01786616\n",
            "Iteration 789, loss = 0.01780353\n",
            "Iteration 790, loss = 0.01777063\n",
            "Iteration 791, loss = 0.01773620\n",
            "Iteration 792, loss = 0.01766636\n",
            "Iteration 793, loss = 0.01760480\n",
            "Iteration 794, loss = 0.01758333\n",
            "Iteration 795, loss = 0.01755954\n",
            "Iteration 796, loss = 0.01751005\n",
            "Iteration 797, loss = 0.01743082\n",
            "Iteration 798, loss = 0.01739375\n",
            "Iteration 799, loss = 0.01737189\n",
            "Iteration 800, loss = 0.01733686\n",
            "Iteration 801, loss = 0.01726151\n",
            "Iteration 802, loss = 0.01725240\n",
            "Iteration 803, loss = 0.01719415\n",
            "Iteration 804, loss = 0.01715206\n",
            "Iteration 805, loss = 0.01710333\n",
            "Iteration 806, loss = 0.01705653\n",
            "Iteration 807, loss = 0.01704957\n",
            "Iteration 808, loss = 0.01695508\n",
            "Iteration 809, loss = 0.01705408\n",
            "Iteration 810, loss = 0.01691972\n",
            "Iteration 811, loss = 0.01692969\n",
            "Iteration 812, loss = 0.01682951\n",
            "Iteration 813, loss = 0.01677807\n",
            "Iteration 814, loss = 0.01675916\n",
            "Iteration 815, loss = 0.01671293\n",
            "Iteration 816, loss = 0.01664100\n",
            "Iteration 817, loss = 0.01658139\n",
            "Iteration 818, loss = 0.01656436\n",
            "Iteration 819, loss = 0.01656451\n",
            "Iteration 820, loss = 0.01649249\n",
            "Iteration 821, loss = 0.01644925\n",
            "Iteration 822, loss = 0.01640060\n",
            "Iteration 823, loss = 0.01634863\n",
            "Iteration 824, loss = 0.01632091\n",
            "Iteration 825, loss = 0.01626957\n",
            "Iteration 826, loss = 0.01625011\n",
            "Iteration 827, loss = 0.01627959\n",
            "Iteration 828, loss = 0.01616259\n",
            "Iteration 829, loss = 0.01620373\n",
            "Iteration 830, loss = 0.01613066\n",
            "Iteration 831, loss = 0.01607293\n",
            "Iteration 832, loss = 0.01602099\n",
            "Iteration 833, loss = 0.01594189\n",
            "Iteration 834, loss = 0.01595882\n",
            "Iteration 835, loss = 0.01591133\n",
            "Iteration 836, loss = 0.01584941\n",
            "Iteration 837, loss = 0.01580452\n",
            "Iteration 838, loss = 0.01578661\n",
            "Iteration 839, loss = 0.01579688\n",
            "Iteration 840, loss = 0.01569888\n",
            "Iteration 841, loss = 0.01569074\n",
            "Iteration 842, loss = 0.01562281\n",
            "Iteration 843, loss = 0.01556998\n",
            "Iteration 844, loss = 0.01552458\n",
            "Iteration 845, loss = 0.01549882\n",
            "Iteration 846, loss = 0.01551677\n",
            "Iteration 847, loss = 0.01542347\n",
            "Iteration 848, loss = 0.01540064\n",
            "Iteration 849, loss = 0.01538463\n",
            "Iteration 850, loss = 0.01532494\n",
            "Iteration 851, loss = 0.01527889\n",
            "Iteration 852, loss = 0.01525915\n",
            "Iteration 853, loss = 0.01522477\n",
            "Iteration 854, loss = 0.01521379\n",
            "Iteration 855, loss = 0.01515540\n",
            "Iteration 856, loss = 0.01514370\n",
            "Iteration 857, loss = 0.01513768\n",
            "Iteration 858, loss = 0.01508009\n",
            "Iteration 859, loss = 0.01501288\n",
            "Iteration 860, loss = 0.01497254\n",
            "Iteration 861, loss = 0.01490775\n",
            "Iteration 862, loss = 0.01489086\n",
            "Iteration 863, loss = 0.01486282\n",
            "Iteration 864, loss = 0.01485033\n",
            "Iteration 865, loss = 0.01476798\n",
            "Iteration 866, loss = 0.01478669\n",
            "Iteration 867, loss = 0.01473850\n",
            "Iteration 868, loss = 0.01470107\n",
            "Iteration 869, loss = 0.01465585\n",
            "Iteration 870, loss = 0.01459170\n",
            "Iteration 871, loss = 0.01460235\n",
            "Iteration 872, loss = 0.01461411\n",
            "Iteration 873, loss = 0.01452299\n",
            "Iteration 874, loss = 0.01448031\n",
            "Iteration 875, loss = 0.01446167\n",
            "Iteration 876, loss = 0.01441625\n",
            "Iteration 877, loss = 0.01438319\n",
            "Iteration 878, loss = 0.01431908\n",
            "Iteration 879, loss = 0.01435583\n",
            "Iteration 880, loss = 0.01431708\n",
            "Iteration 881, loss = 0.01443104\n",
            "Iteration 882, loss = 0.01426407\n",
            "Iteration 883, loss = 0.01422857\n",
            "Iteration 884, loss = 0.01418591\n",
            "Iteration 885, loss = 0.01417302\n",
            "Iteration 886, loss = 0.01410426\n",
            "Iteration 887, loss = 0.01404775\n",
            "Iteration 888, loss = 0.01411053\n",
            "Iteration 889, loss = 0.01399243\n",
            "Iteration 890, loss = 0.01394757\n",
            "Iteration 891, loss = 0.01393977\n",
            "Iteration 892, loss = 0.01387449\n",
            "Iteration 893, loss = 0.01388359\n",
            "Iteration 894, loss = 0.01383629\n",
            "Iteration 895, loss = 0.01380774\n",
            "Iteration 896, loss = 0.01376917\n",
            "Iteration 897, loss = 0.01383512\n",
            "Iteration 898, loss = 0.01375034\n",
            "Iteration 899, loss = 0.01368584\n",
            "Iteration 900, loss = 0.01363915\n",
            "Iteration 901, loss = 0.01361425\n",
            "Iteration 902, loss = 0.01357735\n",
            "Iteration 903, loss = 0.01357282\n",
            "Iteration 904, loss = 0.01351260\n",
            "Iteration 905, loss = 0.01349026\n",
            "Iteration 906, loss = 0.01347775\n",
            "Iteration 907, loss = 0.01340423\n",
            "Iteration 908, loss = 0.01344691\n",
            "Iteration 909, loss = 0.01340478\n",
            "Iteration 910, loss = 0.01332122\n",
            "Iteration 911, loss = 0.01337463\n",
            "Iteration 912, loss = 0.01335443\n",
            "Iteration 913, loss = 0.01326104\n",
            "Iteration 914, loss = 0.01321436\n",
            "Iteration 915, loss = 0.01324749\n",
            "Iteration 916, loss = 0.01316583\n",
            "Iteration 917, loss = 0.01318061\n",
            "Iteration 918, loss = 0.01315789\n",
            "Iteration 919, loss = 0.01308408\n",
            "Iteration 920, loss = 0.01305459\n",
            "Iteration 921, loss = 0.01301982\n",
            "Iteration 922, loss = 0.01299091\n",
            "Iteration 923, loss = 0.01295827\n",
            "Iteration 924, loss = 0.01294520\n",
            "Iteration 925, loss = 0.01290909\n",
            "Iteration 926, loss = 0.01287807\n",
            "Iteration 927, loss = 0.01284653\n",
            "Iteration 928, loss = 0.01280898\n",
            "Iteration 929, loss = 0.01277532\n",
            "Iteration 930, loss = 0.01284466\n",
            "Iteration 931, loss = 0.01275219\n",
            "Iteration 932, loss = 0.01271218\n",
            "Iteration 933, loss = 0.01266462\n",
            "Iteration 934, loss = 0.01265882\n",
            "Iteration 935, loss = 0.01263699\n",
            "Iteration 936, loss = 0.01261016\n",
            "Iteration 937, loss = 0.01261910\n",
            "Iteration 938, loss = 0.01255686\n",
            "Iteration 939, loss = 0.01250981\n",
            "Iteration 940, loss = 0.01253885\n",
            "Iteration 941, loss = 0.01247651\n",
            "Iteration 942, loss = 0.01244564\n",
            "Iteration 943, loss = 0.01242693\n",
            "Iteration 944, loss = 0.01238276\n",
            "Iteration 945, loss = 0.01234372\n",
            "Iteration 946, loss = 0.01232178\n",
            "Iteration 947, loss = 0.01245482\n",
            "Iteration 948, loss = 0.01228803\n",
            "Iteration 949, loss = 0.01225353\n",
            "Iteration 950, loss = 0.01225698\n",
            "Iteration 951, loss = 0.01228020\n",
            "Iteration 952, loss = 0.01220621\n",
            "Iteration 953, loss = 0.01215074\n",
            "Iteration 954, loss = 0.01212500\n",
            "Iteration 955, loss = 0.01211665\n",
            "Iteration 956, loss = 0.01209186\n",
            "Iteration 957, loss = 0.01207109\n",
            "Iteration 958, loss = 0.01204605\n",
            "Iteration 959, loss = 0.01197384\n",
            "Iteration 960, loss = 0.01196070\n",
            "Iteration 961, loss = 0.01194413\n",
            "Iteration 962, loss = 0.01188727\n",
            "Iteration 963, loss = 0.01187949\n",
            "Iteration 964, loss = 0.01185604\n",
            "Iteration 965, loss = 0.01184866\n",
            "Iteration 966, loss = 0.01186193\n",
            "Iteration 967, loss = 0.01179382\n",
            "Iteration 968, loss = 0.01177248\n",
            "Iteration 969, loss = 0.01172737\n",
            "Iteration 970, loss = 0.01170603\n",
            "Iteration 971, loss = 0.01171354\n",
            "Iteration 972, loss = 0.01166656\n",
            "Iteration 973, loss = 0.01165566\n",
            "Iteration 974, loss = 0.01160870\n",
            "Iteration 975, loss = 0.01160997\n",
            "Iteration 976, loss = 0.01159680\n",
            "Iteration 977, loss = 0.01161135\n",
            "Iteration 978, loss = 0.01155650\n",
            "Iteration 979, loss = 0.01147434\n",
            "Iteration 980, loss = 0.01146138\n",
            "Iteration 981, loss = 0.01148123\n",
            "Iteration 982, loss = 0.01144969\n",
            "Iteration 983, loss = 0.01141796\n",
            "Iteration 984, loss = 0.01141856\n",
            "Iteration 985, loss = 0.01139130\n",
            "Iteration 986, loss = 0.01138123\n",
            "Iteration 987, loss = 0.01130919\n",
            "Iteration 988, loss = 0.01131070\n",
            "Iteration 989, loss = 0.01126710\n",
            "Iteration 990, loss = 0.01125286\n",
            "Iteration 991, loss = 0.01121475\n",
            "Iteration 992, loss = 0.01121508\n",
            "Iteration 993, loss = 0.01119129\n",
            "Iteration 994, loss = 0.01117169\n",
            "Iteration 995, loss = 0.01110077\n",
            "Iteration 996, loss = 0.01110374\n",
            "Iteration 997, loss = 0.01107238\n",
            "Iteration 998, loss = 0.01104499\n",
            "Iteration 999, loss = 0.01104830\n",
            "Iteration 1000, loss = 0.01100835\n",
            "Iteration 1001, loss = 0.01108807\n",
            "Iteration 1002, loss = 0.01098441\n",
            "Iteration 1003, loss = 0.01095652\n",
            "Iteration 1004, loss = 0.01094022\n",
            "Iteration 1005, loss = 0.01094176\n",
            "Iteration 1006, loss = 0.01089493\n",
            "Iteration 1007, loss = 0.01081641\n",
            "Iteration 1008, loss = 0.01085795\n",
            "Iteration 1009, loss = 0.01081339\n",
            "Iteration 1010, loss = 0.01080563\n",
            "Iteration 1011, loss = 0.01079006\n",
            "Iteration 1012, loss = 0.01079507\n",
            "Iteration 1013, loss = 0.01071675\n",
            "Iteration 1014, loss = 0.01069346\n",
            "Iteration 1015, loss = 0.01071238\n",
            "Iteration 1016, loss = 0.01068200\n",
            "Iteration 1017, loss = 0.01065152\n",
            "Iteration 1018, loss = 0.01071332\n",
            "Iteration 1019, loss = 0.01063798\n",
            "Iteration 1020, loss = 0.01057466\n",
            "Iteration 1021, loss = 0.01055969\n",
            "Iteration 1022, loss = 0.01050477\n",
            "Iteration 1023, loss = 0.01053196\n",
            "Iteration 1024, loss = 0.01049609\n",
            "Iteration 1025, loss = 0.01046675\n",
            "Iteration 1026, loss = 0.01046689\n",
            "Iteration 1027, loss = 0.01045011\n",
            "Iteration 1028, loss = 0.01042782\n",
            "Iteration 1029, loss = 0.01038871\n",
            "Iteration 1030, loss = 0.01038038\n",
            "Iteration 1031, loss = 0.01033696\n",
            "Iteration 1032, loss = 0.01033774\n",
            "Iteration 1033, loss = 0.01030800\n",
            "Iteration 1034, loss = 0.01039278\n",
            "Iteration 1035, loss = 0.01026829\n",
            "Iteration 1036, loss = 0.01025910\n",
            "Iteration 1037, loss = 0.01026760\n",
            "Iteration 1038, loss = 0.01020339\n",
            "Iteration 1039, loss = 0.01020551\n",
            "Iteration 1040, loss = 0.01023454\n",
            "Iteration 1041, loss = 0.01015796\n",
            "Iteration 1042, loss = 0.01012684\n",
            "Iteration 1043, loss = 0.01013189\n",
            "Iteration 1044, loss = 0.01009181\n",
            "Iteration 1045, loss = 0.01007241\n",
            "Iteration 1046, loss = 0.01004213\n",
            "Iteration 1047, loss = 0.01002959\n",
            "Iteration 1048, loss = 0.01005539\n",
            "Iteration 1049, loss = 0.01002193\n",
            "Iteration 1050, loss = 0.01001263\n",
            "Iteration 1051, loss = 0.00996309\n",
            "Iteration 1052, loss = 0.00993398\n",
            "Iteration 1053, loss = 0.00994418\n",
            "Iteration 1054, loss = 0.00991890\n",
            "Iteration 1055, loss = 0.00985324\n",
            "Iteration 1056, loss = 0.00989083\n",
            "Iteration 1057, loss = 0.00984176\n",
            "Iteration 1058, loss = 0.00982339\n",
            "Iteration 1059, loss = 0.00981593\n",
            "Iteration 1060, loss = 0.00981434\n",
            "Iteration 1061, loss = 0.00977083\n",
            "Iteration 1062, loss = 0.00973888\n",
            "Iteration 1063, loss = 0.00973963\n",
            "Iteration 1064, loss = 0.00976687\n",
            "Iteration 1065, loss = 0.00971080\n",
            "Iteration 1066, loss = 0.00967839\n",
            "Iteration 1067, loss = 0.00965464\n",
            "Iteration 1068, loss = 0.00964596\n",
            "Iteration 1069, loss = 0.00962913\n",
            "Iteration 1070, loss = 0.00961932\n",
            "Iteration 1071, loss = 0.00959952\n",
            "Iteration 1072, loss = 0.00954898\n",
            "Iteration 1073, loss = 0.00954093\n",
            "Iteration 1074, loss = 0.00954560\n",
            "Iteration 1075, loss = 0.00950697\n",
            "Iteration 1076, loss = 0.00951855\n",
            "Iteration 1077, loss = 0.00948588\n",
            "Iteration 1078, loss = 0.00949835\n",
            "Iteration 1079, loss = 0.00941970\n",
            "Iteration 1080, loss = 0.00943345\n",
            "Iteration 1081, loss = 0.00941570\n",
            "Iteration 1082, loss = 0.00943638\n",
            "Iteration 1083, loss = 0.00940447\n",
            "Iteration 1084, loss = 0.00938922\n",
            "Iteration 1085, loss = 0.00933911\n",
            "Iteration 1086, loss = 0.00934708\n",
            "Iteration 1087, loss = 0.00931992\n",
            "Iteration 1088, loss = 0.00929975\n",
            "Iteration 1089, loss = 0.00927856\n",
            "Iteration 1090, loss = 0.00925084\n",
            "Iteration 1091, loss = 0.00921148\n",
            "Iteration 1092, loss = 0.00921072\n",
            "Iteration 1093, loss = 0.00920259\n",
            "Iteration 1094, loss = 0.00918228\n",
            "Iteration 1095, loss = 0.00916075\n",
            "Iteration 1096, loss = 0.00913058\n",
            "Iteration 1097, loss = 0.00913472\n",
            "Iteration 1098, loss = 0.00909033\n",
            "Iteration 1099, loss = 0.00909835\n",
            "Iteration 1100, loss = 0.00906468\n",
            "Iteration 1101, loss = 0.00906512\n",
            "Iteration 1102, loss = 0.00902948\n",
            "Iteration 1103, loss = 0.00905356\n",
            "Iteration 1104, loss = 0.00901931\n",
            "Iteration 1105, loss = 0.00900303\n",
            "Iteration 1106, loss = 0.00898576\n",
            "Iteration 1107, loss = 0.00897433\n",
            "Iteration 1108, loss = 0.00893081\n",
            "Iteration 1109, loss = 0.00892595\n",
            "Iteration 1110, loss = 0.00894050\n",
            "Iteration 1111, loss = 0.00889826\n",
            "Iteration 1112, loss = 0.00892467\n",
            "Iteration 1113, loss = 0.00885784\n",
            "Iteration 1114, loss = 0.00884035\n",
            "Iteration 1115, loss = 0.00884967\n",
            "Iteration 1116, loss = 0.00885027\n",
            "Iteration 1117, loss = 0.00881356\n",
            "Iteration 1118, loss = 0.00878563\n",
            "Iteration 1119, loss = 0.00877317\n",
            "Iteration 1120, loss = 0.00882197\n",
            "Iteration 1121, loss = 0.00874429\n",
            "Iteration 1122, loss = 0.00877585\n",
            "Iteration 1123, loss = 0.00878450\n",
            "Iteration 1124, loss = 0.00872943\n",
            "Iteration 1125, loss = 0.00870402\n",
            "Iteration 1126, loss = 0.00867276\n",
            "Iteration 1127, loss = 0.00866088\n",
            "Iteration 1128, loss = 0.00863033\n",
            "Iteration 1129, loss = 0.00860720\n",
            "Iteration 1130, loss = 0.00864564\n",
            "Iteration 1131, loss = 0.00856818\n",
            "Iteration 1132, loss = 0.00855951\n",
            "Iteration 1133, loss = 0.00857527\n",
            "Iteration 1134, loss = 0.00853171\n",
            "Iteration 1135, loss = 0.00850258\n",
            "Iteration 1136, loss = 0.00864087\n",
            "Iteration 1137, loss = 0.00851938\n",
            "Iteration 1138, loss = 0.00847549\n",
            "Iteration 1139, loss = 0.00845226\n",
            "Iteration 1140, loss = 0.00844123\n",
            "Iteration 1141, loss = 0.00849585\n",
            "Iteration 1142, loss = 0.00844658\n",
            "Iteration 1143, loss = 0.00841066\n",
            "Iteration 1144, loss = 0.00846339\n",
            "Iteration 1145, loss = 0.00836607\n",
            "Iteration 1146, loss = 0.00835470\n",
            "Iteration 1147, loss = 0.00833925\n",
            "Iteration 1148, loss = 0.00833148\n",
            "Iteration 1149, loss = 0.00831728\n",
            "Iteration 1150, loss = 0.00832781\n",
            "Iteration 1151, loss = 0.00834191\n",
            "Iteration 1152, loss = 0.00827737\n",
            "Iteration 1153, loss = 0.00825410\n",
            "Iteration 1154, loss = 0.00823577\n",
            "Iteration 1155, loss = 0.00826656\n",
            "Iteration 1156, loss = 0.00818862\n",
            "Iteration 1157, loss = 0.00826378\n",
            "Iteration 1158, loss = 0.00821961\n",
            "Iteration 1159, loss = 0.00817164\n",
            "Iteration 1160, loss = 0.00820212\n",
            "Iteration 1161, loss = 0.00813887\n",
            "Iteration 1162, loss = 0.00813683\n",
            "Iteration 1163, loss = 0.00811577\n",
            "Iteration 1164, loss = 0.00819855\n",
            "Iteration 1165, loss = 0.00811741\n",
            "Iteration 1166, loss = 0.00809373\n",
            "Iteration 1167, loss = 0.00809445\n",
            "Iteration 1168, loss = 0.00804037\n",
            "Iteration 1169, loss = 0.00806143\n",
            "Iteration 1170, loss = 0.00805668\n",
            "Iteration 1171, loss = 0.00802862\n",
            "Iteration 1172, loss = 0.00799743\n",
            "Iteration 1173, loss = 0.00799823\n",
            "Iteration 1174, loss = 0.00800697\n",
            "Iteration 1175, loss = 0.00796120\n",
            "Iteration 1176, loss = 0.00794071\n",
            "Iteration 1177, loss = 0.00795092\n",
            "Iteration 1178, loss = 0.00791760\n",
            "Iteration 1179, loss = 0.00792327\n",
            "Iteration 1180, loss = 0.00792426\n",
            "Iteration 1181, loss = 0.00805185\n",
            "Iteration 1182, loss = 0.00791832\n",
            "Iteration 1183, loss = 0.00784530\n",
            "Iteration 1184, loss = 0.00784643\n",
            "Iteration 1185, loss = 0.00781651\n",
            "Iteration 1186, loss = 0.00782439\n",
            "Iteration 1187, loss = 0.00784327\n",
            "Iteration 1188, loss = 0.00777649\n",
            "Iteration 1189, loss = 0.00785244\n",
            "Iteration 1190, loss = 0.00776855\n",
            "Iteration 1191, loss = 0.00779048\n",
            "Iteration 1192, loss = 0.00775026\n",
            "Iteration 1193, loss = 0.00770453\n",
            "Iteration 1194, loss = 0.00771948\n",
            "Iteration 1195, loss = 0.00770230\n",
            "Iteration 1196, loss = 0.00767314\n",
            "Iteration 1197, loss = 0.00766735\n",
            "Iteration 1198, loss = 0.00764835\n",
            "Iteration 1199, loss = 0.00764844\n",
            "Iteration 1200, loss = 0.00762619\n",
            "Iteration 1201, loss = 0.00764996\n",
            "Iteration 1202, loss = 0.00762593\n",
            "Iteration 1203, loss = 0.00763586\n",
            "Iteration 1204, loss = 0.00759623\n",
            "Iteration 1205, loss = 0.00756964\n",
            "Iteration 1206, loss = 0.00753648\n",
            "Iteration 1207, loss = 0.00760148\n",
            "Iteration 1208, loss = 0.00756739\n",
            "Iteration 1209, loss = 0.00751660\n",
            "Iteration 1210, loss = 0.00753296\n",
            "Iteration 1211, loss = 0.00752376\n",
            "Iteration 1212, loss = 0.00748269\n",
            "Iteration 1213, loss = 0.00746450\n",
            "Iteration 1214, loss = 0.00750339\n",
            "Iteration 1215, loss = 0.00747119\n",
            "Iteration 1216, loss = 0.00744440\n",
            "Iteration 1217, loss = 0.00746512\n",
            "Iteration 1218, loss = 0.00743382\n",
            "Iteration 1219, loss = 0.00748539\n",
            "Iteration 1220, loss = 0.00738798\n",
            "Iteration 1221, loss = 0.00748434\n",
            "Iteration 1222, loss = 0.00729740\n",
            "Iteration 1223, loss = 0.00745915\n",
            "Iteration 1224, loss = 0.00739431\n",
            "Iteration 1225, loss = 0.00736399\n",
            "Iteration 1226, loss = 0.00732973\n",
            "Iteration 1227, loss = 0.00731221\n",
            "Iteration 1228, loss = 0.00728997\n",
            "Iteration 1229, loss = 0.00728687\n",
            "Iteration 1230, loss = 0.00729599\n",
            "Iteration 1231, loss = 0.00738381\n",
            "Iteration 1232, loss = 0.00720756\n",
            "Iteration 1233, loss = 0.00737488\n",
            "Iteration 1234, loss = 0.00727899\n",
            "Iteration 1235, loss = 0.00723673\n",
            "Iteration 1236, loss = 0.00735930\n",
            "Iteration 1237, loss = 0.00721279\n",
            "Iteration 1238, loss = 0.00720800\n",
            "Iteration 1239, loss = 0.00717234\n",
            "Iteration 1240, loss = 0.00714273\n",
            "Iteration 1241, loss = 0.00719479\n",
            "Iteration 1242, loss = 0.00709897\n",
            "Iteration 1243, loss = 0.00710092\n",
            "Iteration 1244, loss = 0.00715452\n",
            "Iteration 1245, loss = 0.00714167\n",
            "Iteration 1246, loss = 0.00708294\n",
            "Iteration 1247, loss = 0.00707606\n",
            "Iteration 1248, loss = 0.00710976\n",
            "Iteration 1249, loss = 0.00707672\n",
            "Iteration 1250, loss = 0.00710322\n",
            "Iteration 1251, loss = 0.00706515\n",
            "Iteration 1252, loss = 0.00703677\n",
            "Iteration 1253, loss = 0.00709259\n",
            "Iteration 1254, loss = 0.00705941\n",
            "Iteration 1255, loss = 0.00698204\n",
            "Iteration 1256, loss = 0.00699494\n",
            "Iteration 1257, loss = 0.00698051\n",
            "Iteration 1258, loss = 0.00695655\n",
            "Iteration 1259, loss = 0.00695960\n",
            "Iteration 1260, loss = 0.00694784\n",
            "Iteration 1261, loss = 0.00700192\n",
            "Iteration 1262, loss = 0.00690702\n",
            "Iteration 1263, loss = 0.00693733\n",
            "Iteration 1264, loss = 0.00688837\n",
            "Iteration 1265, loss = 0.00691478\n",
            "Iteration 1266, loss = 0.00688506\n",
            "Iteration 1267, loss = 0.00686957\n",
            "Iteration 1268, loss = 0.00691598\n",
            "Iteration 1269, loss = 0.00688097\n",
            "Iteration 1270, loss = 0.00686443\n",
            "Iteration 1271, loss = 0.00686808\n",
            "Iteration 1272, loss = 0.00687191\n",
            "Iteration 1273, loss = 0.00682903\n",
            "Iteration 1274, loss = 0.00680463\n",
            "Iteration 1275, loss = 0.00678554\n",
            "Iteration 1276, loss = 0.00681489\n",
            "Iteration 1277, loss = 0.00675916\n",
            "Iteration 1278, loss = 0.00676010\n",
            "Iteration 1279, loss = 0.00674985\n",
            "Iteration 1280, loss = 0.00673600\n",
            "Iteration 1281, loss = 0.00673257\n",
            "Iteration 1282, loss = 0.00671099\n",
            "Iteration 1283, loss = 0.00669502\n",
            "Iteration 1284, loss = 0.00672164\n",
            "Iteration 1285, loss = 0.00668540\n",
            "Iteration 1286, loss = 0.00667465\n",
            "Iteration 1287, loss = 0.00670000\n",
            "Iteration 1288, loss = 0.00668528\n",
            "Iteration 1289, loss = 0.00667509\n",
            "Iteration 1290, loss = 0.00666051\n",
            "Iteration 1291, loss = 0.00662187\n",
            "Iteration 1292, loss = 0.00662585\n",
            "Iteration 1293, loss = 0.00660678\n",
            "Iteration 1294, loss = 0.00660527\n",
            "Iteration 1295, loss = 0.00657811\n",
            "Iteration 1296, loss = 0.00658061\n",
            "Iteration 1297, loss = 0.00657842\n",
            "Iteration 1298, loss = 0.00657199\n",
            "Iteration 1299, loss = 0.00662960\n",
            "Iteration 1300, loss = 0.00656114\n",
            "Iteration 1301, loss = 0.00655132\n",
            "Iteration 1302, loss = 0.00655797\n",
            "Iteration 1303, loss = 0.00652600\n",
            "Iteration 1304, loss = 0.00655820\n",
            "Iteration 1305, loss = 0.00652082\n",
            "Iteration 1306, loss = 0.00649099\n",
            "Iteration 1307, loss = 0.00649376\n",
            "Iteration 1308, loss = 0.00650823\n",
            "Iteration 1309, loss = 0.00649872\n",
            "Iteration 1310, loss = 0.00645644\n",
            "Iteration 1311, loss = 0.00648860\n",
            "Iteration 1312, loss = 0.00642937\n",
            "Iteration 1313, loss = 0.00644963\n",
            "Iteration 1314, loss = 0.00642770\n",
            "Iteration 1315, loss = 0.00643772\n",
            "Iteration 1316, loss = 0.00639288\n",
            "Iteration 1317, loss = 0.00640190\n",
            "Iteration 1318, loss = 0.00642586\n",
            "Iteration 1319, loss = 0.00641182\n",
            "Iteration 1320, loss = 0.00637241\n",
            "Iteration 1321, loss = 0.00635611\n",
            "Iteration 1322, loss = 0.00634204\n",
            "Iteration 1323, loss = 0.00638642\n",
            "Iteration 1324, loss = 0.00632602\n",
            "Iteration 1325, loss = 0.00637804\n",
            "Iteration 1326, loss = 0.00632608\n",
            "Iteration 1327, loss = 0.00628804\n",
            "Iteration 1328, loss = 0.00632788\n",
            "Iteration 1329, loss = 0.00626935\n",
            "Iteration 1330, loss = 0.00627931\n",
            "Iteration 1331, loss = 0.00628449\n",
            "Iteration 1332, loss = 0.00633931\n",
            "Iteration 1333, loss = 0.00627864\n",
            "Iteration 1334, loss = 0.00631583\n",
            "Iteration 1335, loss = 0.00623342\n",
            "Iteration 1336, loss = 0.00620035\n",
            "Iteration 1337, loss = 0.00622287\n",
            "Iteration 1338, loss = 0.00626216\n",
            "Iteration 1339, loss = 0.00618599\n",
            "Iteration 1340, loss = 0.00619545\n",
            "Iteration 1341, loss = 0.00620483\n",
            "Iteration 1342, loss = 0.00622503\n",
            "Iteration 1343, loss = 0.00618078\n",
            "Iteration 1344, loss = 0.00615747\n",
            "Iteration 1345, loss = 0.00614980\n",
            "Iteration 1346, loss = 0.00616475\n",
            "Iteration 1347, loss = 0.00617414\n",
            "Iteration 1348, loss = 0.00615232\n",
            "Iteration 1349, loss = 0.00612108\n",
            "Iteration 1350, loss = 0.00616300\n",
            "Iteration 1351, loss = 0.00610166\n",
            "Iteration 1352, loss = 0.00613866\n",
            "Iteration 1353, loss = 0.00612236\n",
            "Iteration 1354, loss = 0.00608394\n",
            "Iteration 1355, loss = 0.00616058\n",
            "Iteration 1356, loss = 0.00609469\n",
            "Iteration 1357, loss = 0.00607236\n",
            "Iteration 1358, loss = 0.00609843\n",
            "Iteration 1359, loss = 0.00603985\n",
            "Iteration 1360, loss = 0.00602360\n",
            "Iteration 1361, loss = 0.00604976\n",
            "Iteration 1362, loss = 0.00600919\n",
            "Iteration 1363, loss = 0.00601128\n",
            "Iteration 1364, loss = 0.00599112\n",
            "Iteration 1365, loss = 0.00604842\n",
            "Iteration 1366, loss = 0.00605835\n",
            "Iteration 1367, loss = 0.00599523\n",
            "Iteration 1368, loss = 0.00603357\n",
            "Iteration 1369, loss = 0.00596840\n",
            "Iteration 1370, loss = 0.00597114\n",
            "Iteration 1371, loss = 0.00595451\n",
            "Iteration 1372, loss = 0.00597530\n",
            "Iteration 1373, loss = 0.00599122\n",
            "Iteration 1374, loss = 0.00591497\n",
            "Iteration 1375, loss = 0.00591547\n",
            "Iteration 1376, loss = 0.00591453\n",
            "Iteration 1377, loss = 0.00585680\n",
            "Iteration 1378, loss = 0.00590466\n",
            "Iteration 1379, loss = 0.00592597\n",
            "Iteration 1380, loss = 0.00586485\n",
            "Iteration 1381, loss = 0.00593279\n",
            "Iteration 1382, loss = 0.00586955\n",
            "Iteration 1383, loss = 0.00589138\n",
            "Iteration 1384, loss = 0.00587768\n",
            "Iteration 1385, loss = 0.00583374\n",
            "Iteration 1386, loss = 0.00584482\n",
            "Iteration 1387, loss = 0.00584452\n",
            "Iteration 1388, loss = 0.00578853\n",
            "Iteration 1389, loss = 0.00582602\n",
            "Iteration 1390, loss = 0.00582329\n",
            "Iteration 1391, loss = 0.00578233\n",
            "Iteration 1392, loss = 0.00577691\n",
            "Iteration 1393, loss = 0.00579114\n",
            "Iteration 1394, loss = 0.00577537\n",
            "Iteration 1395, loss = 0.00576066\n",
            "Iteration 1396, loss = 0.00575310\n",
            "Iteration 1397, loss = 0.00575737\n",
            "Iteration 1398, loss = 0.00579922\n",
            "Iteration 1399, loss = 0.00573311\n",
            "Iteration 1400, loss = 0.00571265\n",
            "Iteration 1401, loss = 0.00577410\n",
            "Iteration 1402, loss = 0.00571516\n",
            "Iteration 1403, loss = 0.00567960\n",
            "Iteration 1404, loss = 0.00571578\n",
            "Iteration 1405, loss = 0.00570935\n",
            "Iteration 1406, loss = 0.00566646\n",
            "Iteration 1407, loss = 0.00566529\n",
            "Iteration 1408, loss = 0.00565225\n",
            "Iteration 1409, loss = 0.00573638\n",
            "Iteration 1410, loss = 0.00566575\n",
            "Iteration 1411, loss = 0.00564417\n",
            "Iteration 1412, loss = 0.00564414\n",
            "Iteration 1413, loss = 0.00561687\n",
            "Iteration 1414, loss = 0.00560818\n",
            "Iteration 1415, loss = 0.00563294\n",
            "Iteration 1416, loss = 0.00561093\n",
            "Iteration 1417, loss = 0.00562737\n",
            "Iteration 1418, loss = 0.00563459\n",
            "Iteration 1419, loss = 0.00564388\n",
            "Iteration 1420, loss = 0.00560010\n",
            "Iteration 1421, loss = 0.00557985\n",
            "Iteration 1422, loss = 0.00555853\n",
            "Iteration 1423, loss = 0.00555408\n",
            "Iteration 1424, loss = 0.00555205\n",
            "Iteration 1425, loss = 0.00553957\n",
            "Iteration 1426, loss = 0.00552280\n",
            "Iteration 1427, loss = 0.00553069\n",
            "Iteration 1428, loss = 0.00550286\n",
            "Iteration 1429, loss = 0.00554708\n",
            "Iteration 1430, loss = 0.00551472\n",
            "Iteration 1431, loss = 0.00550481\n",
            "Iteration 1432, loss = 0.00553973\n",
            "Iteration 1433, loss = 0.00550087\n",
            "Iteration 1434, loss = 0.00554699\n",
            "Iteration 1435, loss = 0.00546330\n",
            "Iteration 1436, loss = 0.00544023\n",
            "Iteration 1437, loss = 0.00547712\n",
            "Iteration 1438, loss = 0.00548083\n",
            "Iteration 1439, loss = 0.00547590\n",
            "Iteration 1440, loss = 0.00545503\n",
            "Iteration 1441, loss = 0.00542382\n",
            "Iteration 1442, loss = 0.00542688\n",
            "Iteration 1443, loss = 0.00546326\n",
            "Iteration 1444, loss = 0.00539908\n",
            "Iteration 1445, loss = 0.00542451\n",
            "Iteration 1446, loss = 0.00540888\n",
            "Iteration 1447, loss = 0.00540450\n",
            "Iteration 1448, loss = 0.00539706\n",
            "Iteration 1449, loss = 0.00539345\n",
            "Iteration 1450, loss = 0.00537189\n",
            "Iteration 1451, loss = 0.00537402\n",
            "Iteration 1452, loss = 0.00537160\n",
            "Iteration 1453, loss = 0.00535492\n",
            "Iteration 1454, loss = 0.00535880\n",
            "Iteration 1455, loss = 0.00533125\n",
            "Iteration 1456, loss = 0.00537770\n",
            "Iteration 1457, loss = 0.00538503\n",
            "Iteration 1458, loss = 0.00547092\n",
            "Iteration 1459, loss = 0.00533282\n",
            "Iteration 1460, loss = 0.00532157\n",
            "Iteration 1461, loss = 0.00528513\n",
            "Iteration 1462, loss = 0.00529279\n",
            "Iteration 1463, loss = 0.00530388\n",
            "Iteration 1464, loss = 0.00528814\n",
            "Iteration 1465, loss = 0.00529371\n",
            "Iteration 1466, loss = 0.00528777\n",
            "Iteration 1467, loss = 0.00528246\n",
            "Iteration 1468, loss = 0.00525456\n",
            "Iteration 1469, loss = 0.00525597\n",
            "Iteration 1470, loss = 0.00526307\n",
            "Iteration 1471, loss = 0.00525205\n",
            "Iteration 1472, loss = 0.00526429\n",
            "Iteration 1473, loss = 0.00523799\n",
            "Iteration 1474, loss = 0.00523815\n",
            "Iteration 1475, loss = 0.00524225\n",
            "Iteration 1476, loss = 0.00521832\n",
            "Iteration 1477, loss = 0.00521483\n",
            "Iteration 1478, loss = 0.00520305\n",
            "Iteration 1479, loss = 0.00524486\n",
            "Iteration 1480, loss = 0.00521657\n",
            "Iteration 1481, loss = 0.00517135\n",
            "Iteration 1482, loss = 0.00517181\n",
            "Iteration 1483, loss = 0.00519299\n",
            "Iteration 1484, loss = 0.00518580\n",
            "Iteration 1485, loss = 0.00516252\n",
            "Iteration 1486, loss = 0.00516135\n",
            "Iteration 1487, loss = 0.00519707\n",
            "Iteration 1488, loss = 0.00513672\n",
            "Iteration 1489, loss = 0.00514428\n",
            "Iteration 1490, loss = 0.00515161\n",
            "Iteration 1491, loss = 0.00513028\n",
            "Iteration 1492, loss = 0.00512396\n",
            "Iteration 1493, loss = 0.00514703\n",
            "Iteration 1494, loss = 0.00512694\n",
            "Iteration 1495, loss = 0.00512068\n",
            "Iteration 1496, loss = 0.00509621\n",
            "Iteration 1497, loss = 0.00508392\n",
            "Iteration 1498, loss = 0.00513369\n",
            "Iteration 1499, loss = 0.00507751\n",
            "Iteration 1500, loss = 0.00509911\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-05, verbose=True)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previsoes = rede_neural_credit.predict(X_credit_teste)\n",
        "previsoes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bquIRLA9Wquj",
        "outputId": "e4ead757-cc13-4adc-9d2d-54bc0b882920"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_credit_teste"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJprcDTsWqra",
        "outputId": "ae423f8d-2553-4f4f-e5db-17fa33388362"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "accuracy_score(y_credit_teste, previsoes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3xJhF4RWqpH",
        "outputId": "d72c8091-311d-4aa0-8794-a5b80ae3826b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.998"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "cm = ConfusionMatrix(rede_neural_credit)\n",
        "cm.fit(X_credit_treinamento, y_credit_treinamento)\n",
        "cm.score(X_credit_teste, y_credit_teste)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "r6hhSz1sWqm8",
        "outputId": "3f98d6f1-bcc7-490c-e1e8-b7f7ad8d055b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.998"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_credit_teste, previsoes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1F05hjQ4WqkG",
        "outputId": "cde0eb04-648e-492f-cf77-02e6e15b3353"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       436\n",
            "           1       0.98      1.00      0.99        64\n",
            "\n",
            "    accuracy                           1.00       500\n",
            "   macro avg       0.99      1.00      1.00       500\n",
            "weighted avg       1.00      1.00      1.00       500\n",
            "\n"
          ]
        }
      ]
    }
  ]
}